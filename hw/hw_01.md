---
title: "hw 1: handling the bash"
description: first hw assignment
keywords:
  - bash
  - operating systems
  - file systems
  - scripting
  - file i/o redirection & pipes
---

:::hgroup{.titlegroup}

# hw 1: handling the bash

Andrew Chang-DeWitt \
Math 474, Fall 2025 \
2025 Sept. 9

:::
:::hgroup{.titlegroup}

## task 1

basic linux command line & file system navigation

:::

### part (a)

> Determine your current working directory and display its contents.

### part (b)

> Use a single command to display all files, including hidden ones, in
> your home directory. Elaborate on and explain different arguments
> that his command can take.

### part (c)

> Create a new directory named `assignment_files` in your current location.
> Download the file from
> [https://go.uniwue.de/chaos](https://go.uniwue.de/chaos) using the `wget`
> command, and then extract its contents into the `assignment_files`
> directory. Navigate into the extracted directory and use the find command
> to locate the file named BS.kurs. What is its full path and size?

:::hgroup{.titlegroup}

## task 2

file i/o redirection and pipes

:::

### part (a)

> The command ls NONEXISTENT EXISTENT generates both standard output and an
> error message. Redirect the standard output (stdout) to a file named
> stdout.log and the standard error (stderr) to a separate file named
> stderr.log. Note EXISTENT and NONEXISTENT are 'placeholders' to be filled
> by you.

### part (b)

> Assuming you have files named `A`, `B`, `C`, and `D`, describe the effect
> of the command `cat A- B <C >D`. Be specific about which files are read,
> which are written to, and in what order.

### part (c)

> The `/etc/passwd` file contains a list of all local system users. The
> first column contains the username, and the last column contains the
> user's login shell. Filter this list to only show users who do not have
> `/bin/false` or `/usr/sbin/nologin` as their login shell. Use pipes to
> sort the filtered list alphabetically by the username.

### part (d)

> Save your command history to a file named `hist` using the `history`
> command. Then, using `head` and `tail`, display the first 10 and the last
> 10 commands from the hist file, respectively. Finally, display all
> commands from the `hist` file except for the first 10.

### part (e)

> First, use curl to retrieve the content of `www.google.com`. Then, use `grep` and a regular expression to find all links in the format of `<a href="...">`. Finally, using the `sed` command, prepend the text "URL: " to each matching URL.

:::hgroup{.titlegroup}

## task 3

bash scripting

:::

### part (a)

Create a bash script named `mvall`. This script should accept two or three
parameters:

- The first parameter, if present, can be a flag `--log`. If this flag is
  provided, the script should print a log message after completion.
- The second parameter is a file extension (e.g., `.txt`, `.pdf`).
- The third parameter is the name of a new directory to be created.

Further, the script should check if the new directory already exists. If it
does, the script should exit with an error. It should count the number of
files in the current directory that match the given file extension, create
the new directory, and then move all matching files into it. If the `--log`
flag was provided, the script should print the number of files moved to the
new directory.

:::hgroup{.titlegroup}

## task 4

process management

:::

### part (a)

> Briefly explain the different signals used with the `kill` command (e.g., `TERM`, `INT`, `KILL`) and in what situation you would use each.

### part (b)

> Use the `&` operator to run the `cat /dev/random` command in the background. Note the process ID (`PID`) that is displayed. Then, terminate the background process you started.

:::hgroup{.titlegroup}

## task 5

"Gonna catch 'em all"

:::

> Ash is a computer science student at Illinois Tech and a die-hard Pokémon
> (see [Wikipedia](https://en.wikipedia.org/wiki/Pokémon)) fan. He thinks
> while attending the OS lecture "As I cannot catch Pokémon right now, I can
> gather information on them". To achieve his plan, Ash wants to create a
> web crawler that retrieve all information from the [Poké
> Wiki](https://pokemon.fandom.com/wiki/List_of_Pokémon). The crawler should
> recursively visit the associated links, but ultimately each link only
> once. Ash also wants to appropriately visualize the resulting page
> hierarchy using a tree graph.

> Translated, the text above means that this final task requires writing a
> compact web crawler. It should recursively call all possible subpages from
> the any website (you are free to use the Pokémon Wiki), but in such a way
> that each link is visited only once in the end. In addition, the resulting
> hierarchy should be appropriately visualized using a tree graph.

### part (a)

> First, write a script that parses the content of a given `URL` and outputs
> all links. The script accepts either 1 or 2 arguments. If only 1 argument
> is passed, it is the domain. With two argu ents, the first argument is the
> domain and the second is the path. This means that when calling `script.sh
www.test.com` and `script.sh www.test.de/path1/path2`, only the page
> `www.test.com` is read, but when calling `script.sh www.test.com path1`,
> the page `www.test.com/path1` is read. After all links from the page have
> been captured, only the URLs that start with `http[s]` should be
> considered for simplicity.

### part (b)

> The second script should control the crawling. It starts with the staring
> website (e.g., `https://pokemon.fandom.com/wiki/List_of_Pokémon`) (Level
> 0). The links there are read with the script from task a). These links
> (Level 1) should be saved to a file, e.g., `level1.txt`. Further more, for
> each extracted link, all links reachable from there are read out again.
> These new links (Level 2), which are all reachable from the 'old' links
> (Level 1), should also be saved to a file, e.g., `level2.txt`. In short,
> all links reachable from Level i (Level i+1) are written out. To avoid
> infinite loops, the script must remember which URLs it has already read
> and then skip them. In addition, only pages that start with the same
> domain (in the example `https://pokemon.fandom.com/`) should be read.

### part (c)

> Write a script that visualizes (as tree graph) for a given level from
> which URLs other URLs are reachable.

### part (d)

> Name your favorite Pokémon and elaborate why.
